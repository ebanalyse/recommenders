{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfm import pylibfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyfm import pylibfm\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.sparse as ss\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/v-taqi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_news(path,filename):\n",
    "    news=['']\n",
    "    category=[]\n",
    "    subcategory=[]\n",
    "    news_index={}\n",
    "    index=1\n",
    "    word_dict={}\n",
    "    word_index=1\n",
    "    with open(os.path.join(path,filename)) as f:\n",
    "        lines=f.readlines()\n",
    "    for line in lines:\n",
    "        doc_id,vert,subvert,title=line.strip('\\n').split('\\t')[0:4]\n",
    "        news_index[doc_id]=index\n",
    "        index+=1\n",
    "        category.append(vert)\n",
    "        subcategory.append(subvert)\n",
    "        news.append(title)\n",
    "\n",
    "    return news,news_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_path = './V2Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "news,news_index= read_news(data_root_path,'docs.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_clickhistory(path,filename):\n",
    "    with open(os.path.join(path,filename)) as f:\n",
    "        lines=f.readlines()\n",
    "    session=[]\n",
    "    for l in lines:\n",
    "        userid, clicks, imp =l.strip().split('\\t')\n",
    "        clicks = clicks.split('#N#')\n",
    "        true_click = []\n",
    "        for click in clicks:\n",
    "            t = click.split('#TAB#')[0]\n",
    "            if t =='':\n",
    "                continue\n",
    "            true_click.append(t)\n",
    "        pos, neg, _ = imp.split('#TAB#')\n",
    "        pos = pos.split()\n",
    "        neg = neg.split()\n",
    "        \n",
    "        session.append([true_click,pos,neg])\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_session=read_clickhistory(data_root_path,'train.tsv')\n",
    "test_session=read_clickhistory(data_root_path,'test.tsv')\n",
    "val_session=read_clickhistory(data_root_path,'val.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_user(session,news_index,news):\n",
    "    users = []\n",
    "    for i in range(len(session)):\n",
    "        clicked = ''\n",
    "        for j in range(len(session[i][0])):\n",
    "            h =session[i][0][j]\n",
    "            index = news_index[h]\n",
    "            clicked+=news[index]+' '\n",
    "        users.append(clicked)\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = parser_user(train_session,news_index,news)\n",
    "test_user = parser_user(test_session,news_index,news)\n",
    "val_user = parser_user(val_session,news_index,news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfVectorizer(stop_words = stopWords,max_features=12000)\n",
    "NewsVec = transformer.fit(news)\n",
    "news_vec = transformer.transform(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_vec = transformer.transform(train_user)\n",
    "test_user_vec = transformer.transform(test_user)\n",
    "val_user_vec = transformer.transform(val_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsample(nnn,ratio):\n",
    "    if ratio >len(nnn):\n",
    "        return random.sample(nnn*(ratio//len(nnn)+1),ratio)\n",
    "    else:\n",
    "        return random.sample(nnn,ratio)\n",
    "\n",
    "def get_train_session(session,npratio,news_index):\n",
    "    user_id = []\n",
    "    doc_id = []\n",
    "    label_id = []\n",
    "    for sess_id in range(len(session)):\n",
    "        clicked, poss, negs = session[sess_id]\n",
    "        for pos in poss:\n",
    "            news_id = news_index[pos]\n",
    "            user_id.append(sess_id)\n",
    "            doc_id.append(news_id)\n",
    "            label_id.append(1)\n",
    "            sampled_negs = newsample(negs,npratio)\n",
    "            for neg in sampled_negs:\n",
    "                news_id = news_index[neg]\n",
    "                user_id.append(sess_id)\n",
    "                doc_id.append(news_id)\n",
    "                label_id.append(0)\n",
    "    user_id = np.array(user_id,dtype='int32')\n",
    "    doc_id = np.array(doc_id,dtype='int32')\n",
    "    label_id = np.array(label_id)\n",
    "    return user_id, doc_id, label_id\n",
    "\n",
    "def get_test_session(session,news_index):\n",
    "    sess_all = []\n",
    "    user_id = []\n",
    "    label = []\n",
    "    sess_location = []\n",
    "    index = 0\n",
    "    for sess_id in range(len(session)):\n",
    "        sess = session[sess_id]\n",
    "        _,poss,negs=sess\n",
    "        start = index\n",
    "        for pos in poss:\n",
    "            sess_all.append(news_index[pos])\n",
    "            label.append(1.0)\n",
    "            user_id.append(sess_id)\n",
    "            index+=1\n",
    "        for neg in negs:\n",
    "            sess_all.append(news_index[neg])\n",
    "            label.append(0.0)\n",
    "            user_id.append(sess_id)\n",
    "            index+=1\n",
    "        ed = index\n",
    "        sess_location.append([start,ed])\n",
    "    sess_all = np.array(sess_all,dtype='int32')\n",
    "    user_id = np.array(user_id,dtype='int32')\n",
    "    label = np.array(label,dtype='int32')\n",
    "    \n",
    "    return sess_all,label,user_id,sess_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_id, train_doc_id, train_label_id = get_train_session(train_session,1,news_index)\n",
    "test_doc_id, test_label_id, test_user_id, test_impressionid = get_test_session(test_session,news_index)\n",
    "val_doc_id, val_label_id, val_user_id, val_impressionid = get_test_session(val_session,news_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_feature = train_user_vec[train_user_id.astype('int')]\n",
    "train_news_feature = news_vec[train_doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = ss.hstack([train_user_feature,train_news_feature]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = pylibfm.FM(\n",
    "            num_factors=10, \n",
    "            num_iter=1, \n",
    "            verbose=True, \n",
    "            task=\"classification\", \n",
    "            initial_learning_rate=0.001, \n",
    "            learning_rate_schedule=\"optimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = pylibfm.FM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset of 0.01 of training for adaptive regularization\n",
      "-- Epoch 1\n",
      "Training log loss: 0.66477\n",
      "CPU times: user 5min 57s, sys: 1.13 s, total: 5min 59s\n",
      "Wall time: 6min 2s\n"
     ]
    }
   ],
   "source": [
    "%time fm.fit(train_x, train_label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_label = np.zeros(test_label_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_in_batch(test_predict_label):\n",
    "    i = 0\n",
    "    batch_size = 1024\n",
    "    flag = True\n",
    "    while flag:\n",
    "        if i%1000==0:\n",
    "            print(i)\n",
    "        start = i*batch_size\n",
    "        ed = (i+1)*batch_size\n",
    "        i+=1\n",
    "        if ed>=test_predict_label.shape[0]:\n",
    "            flag = False\n",
    "            ed = test_predict_label.shape[0]\n",
    "        batch_test_user_feature = test_user_vec[test_user_id[start:ed].astype('int')]\n",
    "        batch_test_news_feature = news_vec[test_doc_id[start:ed]]\n",
    "        batch_test_x = ss.hstack([batch_test_user_feature,batch_test_news_feature]).tocsr()\n",
    "        test_predict_label[start:ed]=fm.predict(batch_test_x)\n",
    "    return test_predict_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "test_predict_label = compute_test_in_batch(test_predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute(score,test_impressionid):\n",
    "    all_auc=[]\n",
    "    all_mrr=[]\n",
    "    all_ndcg=[]\n",
    "    all_ndcg2=[]\n",
    "    for impression_id in range(len(test_impressionid)):\n",
    "        start,ed=test_impressionid[impression_id]\n",
    "        y_score=score[start:ed]\n",
    "        y_true=test_label[start:ed]\n",
    "            \n",
    "        all_auc.append(roc_auc_score(y_true,y_score))\n",
    "        all_mrr.append(mrr_score(y_true,y_score))\n",
    "        all_ndcg.append(ndcg_score(y_true,y_score,5))\n",
    "        all_ndcg2.append(ndcg_score(y_true,y_score,10))\n",
    "    print(np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2))\n",
    "    return [np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute(score,test_label,test_impressionid):\n",
    "    all_auc=[]\n",
    "    all_mrr=[]\n",
    "    all_ndcg=[]\n",
    "    all_ndcg2=[]\n",
    "    for impression_id in range(len(test_impressionid)):\n",
    "        start,ed=test_impressionid[impression_id]\n",
    "        y_score=score[start:ed]\n",
    "        y_true=test_label[start:ed]\n",
    "        print(impression_id)\n",
    "        all_auc.append(roc_auc_score(y_true,y_score))\n",
    "        all_mrr.append(mrr_score(y_true,y_score))\n",
    "        all_ndcg.append(ndcg_score(y_true,y_score,5))\n",
    "        all_ndcg2.append(ndcg_score(y_true,y_score,10))\n",
    "    print(np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2))\n",
    "    return [np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute(score,test_label,test_impressionid):\n",
    "    all_auc=[]\n",
    "    all_mrr=[]\n",
    "    all_ndcg=[]\n",
    "    all_ndcg2=[]\n",
    "    for impression_id in range(len(test_impressionid)):\n",
    "        start,ed=test_impressionid[impression_id]\n",
    "        y_score=score[start:ed]\n",
    "        y_true=test_label[start:ed]\n",
    "        all_auc.append(roc_auc_score(y_true,y_score))\n",
    "        all_mrr.append(mrr_score(y_true,y_score))\n",
    "        all_ndcg.append(ndcg_score(y_true,y_score,5))\n",
    "        all_ndcg2.append(ndcg_score(y_true,y_score,10))\n",
    "    all_auc = np.array(all_auc)*100\n",
    "    all_mrr = np.array(all_mrr)*100\n",
    "    all_ndcg = np.array(all_ndcg)*100\n",
    "    all_ndcg2 = np.array(all_ndcg2)*100\n",
    "    return all_auc, all_mrr, all_ndcg, all_ndcg2\n",
    "\n",
    "def print_mean(AUC,MRR,n5,n10):\n",
    "    AUC = AUC.mean()\n",
    "    MRR = MRR.mean()\n",
    "    n5 = n5.mean()\n",
    "    n10 = n10.mean()\n",
    "    print(AUC, MRR, n5, n10)\n",
    "    return [AUC, MRR, n5, n10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC, MRR, nDCG5, nDCG10 = evalute(test_predict_label,test_label_id,test_impressionid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.68677383957965 25.836799571121862 27.098444658760297 32.54843744188427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[55.68677383957965, 25.836799571121862, 27.098444658760297, 32.54843744188427]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_mean(AUC, MRR, nDCG5, nDCG10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predict_label = np.zeros(val_label_id.shape,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_val_in_batch():\n",
    "    i = 0\n",
    "    batch_size = 1024\n",
    "    flag = True\n",
    "    while flag:\n",
    "        if i%1000==0:\n",
    "            print(i)\n",
    "        start = i*batch_size\n",
    "        ed = (i+1)*batch_size\n",
    "        i+=1\n",
    "        if ed>=val_predict_label.shape[0]:\n",
    "            flag = False\n",
    "            ed = val_predict_label.shape[0]\n",
    "        batch_val_user_feature = val_user_vec[val_user_id[start:ed].astype('int')]\n",
    "        batch_val_news_feature = news_vec[val_doc_id[start:ed]]\n",
    "        batch_val_x = ss.hstack([batch_val_user_feature,batch_val_news_feature]).tocsr()\n",
    "        t=fm.predict(batch_val_x)\n",
    "        val_predict_label[start:ed] = t\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "compute_val_in_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_AUC, val_MRR, val_nDCG5, val_nDCG10 = evalute(val_predict_label,val_label_id,val_impressionid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.295982350628755 25.681055902498475 27.811220951601094 33.81432924016593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[56.295982350628755, 25.681055902498475, 27.811220951601094, 33.81432924016593]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_mean(val_AUC,val_MRR,val_nDCG5,val_nDCG10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
